{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Quantitative Competition\n",
    "\n",
    "This notebook performs comprehensive EDA on the competition data to understand:\n",
    "- Data distributions and characteristics\n",
    "- Target correlations\n",
    "- Feature relationships\n",
    "- Temporal patterns\n",
    "- Spike features and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "feature_cols = [chr(ord('A') + i) for i in range(14)]  # A through N\n",
    "target_cols = ['Y1', 'Y2']\n",
    "time_col = 'time'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_cols]\n",
    "time = df[time_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"Time range: {time.min()} to {time.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isna().sum(),\n",
    "    'Missing_Percent': (df.isna().sum() / len(df)) * 100\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing values found:\")\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Visualize missing patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isna(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "    plt.title('Missing Values Pattern')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target statistics\n",
    "print(\"Target Statistics:\")\n",
    "print(y.describe())\n",
    "\n",
    "# Target correlation\n",
    "target_corr = y.corr().iloc[0, 1]\n",
    "print(f\"\\nY1-Y2 Correlation: {target_corr:.4f}\")\n",
    "\n",
    "if abs(target_corr) < 0.3:\n",
    "    print(\"⚠️ Low correlation - consider independent models\")\n",
    "elif abs(target_corr) > 0.9:\n",
    "    print(\"⚠️ Very high correlation - check for redundancy\")\n",
    "else:\n",
    "    print(\"✓ Moderate correlation - good for multi-target learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Y1 distribution\n",
    "axes[0, 0].hist(y['Y1'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Y1 Distribution')\n",
    "axes[0, 0].set_xlabel('Y1')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Y2 distribution\n",
    "axes[0, 1].hist(y['Y2'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Y2 Distribution')\n",
    "axes[0, 1].set_xlabel('Y2')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Scatter plot\n",
    "axes[0, 2].scatter(y['Y1'], y['Y2'], alpha=0.5, s=1)\n",
    "axes[0, 2].set_title(f'Y1 vs Y2 (corr={target_corr:.3f})')\n",
    "axes[0, 2].set_xlabel('Y1')\n",
    "axes[0, 2].set_ylabel('Y2')\n",
    "\n",
    "# Q-Q plots\n",
    "stats.probplot(y['Y1'], dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Y1 Q-Q Plot')\n",
    "\n",
    "stats.probplot(y['Y2'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Y2 Q-Q Plot')\n",
    "\n",
    "# Time series of targets\n",
    "axes[1, 2].plot(time[:1000], y['Y1'][:1000], label='Y1', alpha=0.7)\n",
    "axes[1, 2].plot(time[:1000], y['Y2'][:1000], label='Y2', alpha=0.7)\n",
    "axes[1, 2].set_title('Target Time Series (first 1000 samples)')\n",
    "axes[1, 2].set_xlabel('Time')\n",
    "axes[1, 2].set_ylabel('Target Value')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "print(\"Feature Statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "n_features = len(feature_cols)\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    axes[i].hist(X[col].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'Feature {col}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = X[col].mean()\n",
    "    std_val = X[col].std()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_features, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spike Features Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect spike features (high concentration of specific values)\n",
    "spike_threshold = 0.1  # 10% of samples with same value\n",
    "spike_features = []\n",
    "\n",
    "print(\"Spike Features Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in feature_cols:\n",
    "    value_counts = X[col].value_counts()\n",
    "    if len(value_counts) > 0:\n",
    "        top_value = value_counts.iloc[0]\n",
    "        top_value_ratio = top_value / len(X)\n",
    "        \n",
    "        if top_value_ratio > spike_threshold:\n",
    "            spike_features.append(col)\n",
    "            print(f\"Feature {col}:\")\n",
    "            print(f\"  - {top_value_ratio:.1%} samples have value {value_counts.index[0]:.4f}\")\n",
    "            print(f\"  - Top 5 values: {value_counts.head().to_dict()}\")\n",
    "            print()\n",
    "\n",
    "if spike_features:\n",
    "    print(f\"\\nFound {len(spike_features)} spike features: {spike_features}\")\n",
    "    print(\"These features may require special handling (e.g., categorical encoding)\")\n",
    "else:\n",
    "    print(\"No spike features detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature-Target Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with targets\n",
    "correlations = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Corr_Y1': [X[col].corr(y['Y1']) for col in feature_cols],\n",
    "    'Corr_Y2': [X[col].corr(y['Y2']) for col in feature_cols]\n",
    "})\n",
    "\n",
    "correlations['Avg_Abs_Corr'] = (correlations['Corr_Y1'].abs() + correlations['Corr_Y2'].abs()) / 2\n",
    "correlations = correlations.sort_values('Avg_Abs_Corr', ascending=False)\n",
    "\n",
    "print(\"Feature-Target Correlations:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Y1 correlations\n",
    "axes[0].barh(correlations['Feature'], correlations['Corr_Y1'].abs())\n",
    "axes[0].set_xlabel('Absolute Correlation')\n",
    "axes[0].set_title('Feature Correlations with Y1')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Y2 correlations\n",
    "axes[1].barh(correlations['Feature'], correlations['Corr_Y2'].abs())\n",
    "axes[1].set_xlabel('Absolute Correlation')\n",
    "axes[1].set_title('Feature Correlations with Y2')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation matrix\n",
    "feature_corr_matrix = X.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(feature_corr_matrix, dtype=bool))\n",
    "sns.heatmap(feature_corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, \n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        corr_val = abs(feature_corr_matrix.iloc[i, j])\n",
    "        if corr_val > high_corr_threshold:\n",
    "            high_corr_pairs.append((feature_cols[i], feature_cols[j], corr_val))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nHighly correlated feature pairs (|corr| > {high_corr_threshold}):\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(f\"\\nNo feature pairs with |correlation| > {high_corr_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns\n",
    "window_size = 1000\n",
    "n_windows = len(df) // window_size\n",
    "\n",
    "temporal_stats = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    start_idx = i * window_size\n",
    "    end_idx = (i + 1) * window_size\n",
    "    window_data = df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    stats = {\n",
    "        'window': i,\n",
    "        'time_start': window_data['time'].iloc[0],\n",
    "        'time_end': window_data['time'].iloc[-1],\n",
    "        'Y1_mean': window_data['Y1'].mean(),\n",
    "        'Y1_std': window_data['Y1'].std(),\n",
    "        'Y2_mean': window_data['Y2'].mean(),\n",
    "        'Y2_std': window_data['Y2'].std()\n",
    "    }\n",
    "    \n",
    "    # Add feature means\n",
    "    for col in feature_cols[:5]:  # First 5 features for brevity\n",
    "        stats[f'{col}_mean'] = window_data[col].mean()\n",
    "    \n",
    "    temporal_stats.append(stats)\n",
    "\n",
    "temporal_df = pd.DataFrame(temporal_stats)\n",
    "\n",
    "# Plot temporal patterns\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Target means over time\n",
    "axes[0].plot(temporal_df['window'], temporal_df['Y1_mean'], label='Y1 mean', alpha=0.7)\n",
    "axes[0].plot(temporal_df['window'], temporal_df['Y2_mean'], label='Y2 mean', alpha=0.7)\n",
    "axes[0].set_xlabel('Time Window')\n",
    "axes[0].set_ylabel('Mean Value')\n",
    "axes[0].set_title('Target Means Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target volatility over time\n",
    "axes[1].plot(temporal_df['window'], temporal_df['Y1_std'], label='Y1 std', alpha=0.7)\n",
    "axes[1].plot(temporal_df['window'], temporal_df['Y2_std'], label='Y2 std', alpha=0.7)\n",
    "axes[1].set_xlabel('Time Window')\n",
    "axes[1].set_ylabel('Std Deviation')\n",
    "axes[1].set_title('Target Volatility Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature means over time (first 5 features)\n",
    "for col in feature_cols[:5]:\n",
    "    axes[2].plot(temporal_df['window'], temporal_df[f'{col}_mean'], label=f'{col}', alpha=0.7)\n",
    "axes[2].set_xlabel('Time Window')\n",
    "axes[2].set_ylabel('Mean Value')\n",
    "axes[2].set_title('Feature Means Over Time (First 5 Features)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for regime changes\n",
    "print(\"\\nTemporal Statistics Summary:\")\n",
    "print(f\"Y1 mean range: [{temporal_df['Y1_mean'].min():.4f}, {temporal_df['Y1_mean'].max():.4f}]\")\n",
    "print(f\"Y2 mean range: [{temporal_df['Y2_mean'].min():.4f}, {temporal_df['Y2_mean'].max():.4f}]\")\n",
    "print(f\"Y1 volatility range: [{temporal_df['Y1_std'].min():.4f}, {temporal_df['Y1_std'].max():.4f}]\")\n",
    "print(f\"Y2 volatility range: [{temporal_df['Y2_std'].min():.4f}, {temporal_df['Y2_std'].max():.4f}]\")\n",
    "\n",
    "# Check for significant changes\n",
    "y1_mean_change = temporal_df['Y1_mean'].max() - temporal_df['Y1_mean'].min()\n",
    "y2_mean_change = temporal_df['Y2_mean'].max() - temporal_df['Y2_mean'].min()\n",
    "\n",
    "if y1_mean_change > temporal_df['Y1_std'].mean() or y2_mean_change > temporal_df['Y2_std'].mean():\n",
    "    print(\"\\n⚠️ Warning: Significant temporal shifts detected. Consider time-aware modeling.\")\n",
    "else:\n",
    "    print(\"\\n✓ Temporal patterns appear relatively stable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "outlier_summary = []\n",
    "\n",
    "for col in feature_cols + target_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Outlier_Count': len(outliers),\n",
    "        'Outlier_Percent': len(outliers) / len(df) * 100,\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df = outlier_df.sort_values('Outlier_Percent', ascending=False)\n",
    "\n",
    "print(\"Outlier Analysis (IQR Method):\")\n",
    "print(outlier_df)\n",
    "\n",
    "# Visualize outliers for features with most outliers\n",
    "top_outlier_features = outlier_df.head(4)['Column'].tolist()\n",
    "\n",
    "if top_outlier_features:\n",
    "    fig, axes = plt.subplots(1, min(4, len(top_outlier_features)), figsize=(15, 4))\n",
    "    if len(top_outlier_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(top_outlier_features[:4]):\n",
    "        axes[i].boxplot(df[col].dropna())\n",
    "        axes[i].set_title(f'{col} Boxplot')\n",
    "        axes[i].set_ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EDA SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Data characteristics\n",
    "print(\"\\n📊 Data Characteristics:\")\n",
    "print(f\"  - Total samples: {len(df):,}\")\n",
    "print(f\"  - Features: {len(feature_cols)} ({', '.join(feature_cols)})\")\n",
    "print(f\"  - Targets: {len(target_cols)} ({', '.join(target_cols)})\")\n",
    "print(f\"  - Y1-Y2 correlation: {target_corr:.4f}\")\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n🔍 Key Findings:\")\n",
    "\n",
    "# Missing values\n",
    "total_missing = df.isna().sum().sum()\n",
    "if total_missing > 0:\n",
    "    print(f\"  - Missing values: {total_missing} ({total_missing/df.size*100:.2f}% of all data)\")\n",
    "    print(\"    → Implement appropriate imputation strategy\")\n",
    "else:\n",
    "    print(\"  - No missing values detected ✓\")\n",
    "\n",
    "# Spike features\n",
    "if spike_features:\n",
    "    print(f\"  - Spike features found: {spike_features}\")\n",
    "    print(\"    → Consider categorical encoding or special handling\")\n",
    "else:\n",
    "    print(\"  - No significant spike features ✓\")\n",
    "\n",
    "# Feature correlations\n",
    "best_features = correlations.head(5)['Feature'].tolist()\n",
    "print(f\"  - Top correlated features: {best_features}\")\n",
    "print(\"    → Prioritize these in feature engineering\")\n",
    "\n",
    "# Outliers\n",
    "high_outlier_cols = outlier_df[outlier_df['Outlier_Percent'] > 5]['Column'].tolist()\n",
    "if high_outlier_cols:\n",
    "    print(f\"  - High outlier columns (>5%): {high_outlier_cols}\")\n",
    "    print(\"    → Consider robust scaling or outlier treatment\")\n",
    "\n",
    "# Model recommendations\n",
    "print(\"\\n🎯 Modeling Recommendations:\")\n",
    "print(\"  1. Use multi-target learning given Y1-Y2 correlation\")\n",
    "print(\"  2. Implement purged time-series cross-validation\")\n",
    "print(\"  3. Focus on top 30-40 interaction features\")\n",
    "print(\"  4. Apply regularization to prevent overfitting\")\n",
    "print(\"  5. Use ensemble methods for robustness\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\n📝 Next Steps:\")\n",
    "print(\"  1. Implement feature engineering pipeline\")\n",
    "print(\"  2. Set up purged cross-validation\")\n",
    "print(\"  3. Train LightGBM baseline\")\n",
    "print(\"  4. Validate CV score > 0.68\")\n",
    "print(\"  5. Build ensemble if baseline successful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}